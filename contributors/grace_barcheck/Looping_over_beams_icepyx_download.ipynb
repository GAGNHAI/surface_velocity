{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Query icepyx; see what tracks are available in area of interest\n",
    "\n",
    "2. Save track numbers, beams, and repeat numbers into a dictionary\n",
    "\n",
    "3. For each track/beam combination, loop over all possible repeat pairs\n",
    "\n",
    "    A. Load all beams and all repeats for that track using icepyx (?). For all beams / repeats:\n",
    "    \n",
    "        - Do whatever we are doing with ATL03\n",
    "    \n",
    "        - Fill in nan gaps with noise\n",
    "        \n",
    "    B. For each repeat pair:\n",
    "        \n",
    "        - Loop across the along track coordinates: \n",
    "        \n",
    "            Choices: window size, search width, running average window size, step, where to save data geographically\n",
    "            \n",
    "            Output: Best lag, corresponding correlation coefficient, equivalent along-track velocity\n",
    "            \n",
    "        - Save results in a text file with date collected, dx from ATL03 processing, lat, lon, veloc, correlation coefficient, best lag, # contributing nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepyx import icesat2data as ipd\n",
    "import os, glob, re, h5py, sys, pyproj\n",
    "import matplotlib as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from astropy.time import Time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "ATL06_files=glob.glob(os.path.join(datapath, '*.h5'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0080', '1131', '0232', '1031', '0634', '0507', '0131', '0192', '0354', '1061', '0492', '0690', '0970', '0187', '0558', '1335', '0741', '0659', '0894', '1183', '0680', '1101', '1168', '0034', '0568', '0705', '0293', '0711', '1040', '0070', '0543', '1244', '1192', '0314', '0126', '1193', '1147', '0253', '0451', '1122', '0994', '0391', '0141', '0979', '0476', '1223', '1137', '0726', '0918', '1314', '1253', '1177', '0750', '0330', '1010', '0193', '0781', '0872', '1299', '0629', '1055', '0695', '0309', '0467', '0802', '0644', '0461', '0415', '0635', '0924', '0482', '1214', '1076', '0573', '0339', '0833', '0171', '0446', '0385', '1336', '0796', '0369', '0756', '1238', '0674', '0903', '0955', '0650', '0772', '0832', '0766', '0513', '0308', '0857', '0720', '1162', '0848', '0202', '0019', '0071', '1138', '1259', '0522', '0390', '1254', '0360', '0933', '1025', '0512', '1000', '1153', '0842', '0400', '1351', '0751', '0628', '0537', '0583', '0878', '1320', '0491', '0552', '0421', '1315', '1015', '0954', '0040', '1275', '0132', '0812', '0598', '0985', '1016', '1330', '0324', '0589', '1132', '0909', '0049', '0370', '0263', '0735', '0613', '0095', '0431', '0345', '1071', '0689', '1274', '1376', '1345', '0811', '0004', '0888', '0452', '0065', '0964', '0217', '1208', '0893', '0147', '1360', '0939', '1092', '1229', '0110', '1375', '0406', '0817', '0436', '0863', '0787', '0934', '0574', '0949', '0873', '1070', '0497', '0009', '1366', '0528', '1198', '0025', '0619', '0247', '0010', '1107', '1046', '0162', '1305', '1116', '0604', '0827', '0665', '0430', '0116', '0771', '0208', '0086', '0375', '0186', '0248', '0299', '1381', '0284', '0269', '1086', '1290', '0223', '1284', '0177', '0101', '1077', '0156', '0278', '1269', '0238', '0055'])\n",
      "['04', '02', '03', '01']\n"
     ]
    }
   ],
   "source": [
    "rgts = {}\n",
    "for filepath in ATL06_files:\n",
    "    filename = filepath.split('/')[-1]\n",
    "    rgt = filename.split('_')[3][0:4]\n",
    "    track = filename.split('_')[3][4:6]\n",
    "#     print(rgt,track)\n",
    "    if not rgt in rgts.keys():\n",
    "        rgts[rgt] = []\n",
    "        rgts[rgt].append(track)\n",
    "    else:\n",
    "        rgts[rgt].append(track)\n",
    "\n",
    "\n",
    "# all rgt values in our study are are in rgts.keys()\n",
    "print(rgts.keys())\n",
    "\n",
    "# available tracks for each rgt are in rgts[rgt]; ex.:\n",
    "print(rgts['0848'])\n",
    "\n",
    "# let's work 0848, our first good track friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:27: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:27: DeprecationWarning: invalid escape sequence \\d\n",
      "<ipython-input-96-70e50fb9eb67>:27: DeprecationWarning: invalid escape sequence \\d\n",
      "  file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n"
     ]
    }
   ],
   "source": [
    "def atl06_to_dict(filename, beam, field_dict=None, index=None, epsg=None):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            beam: a string specifying which beam is to be read (ex: gt1l, gt1r, gt2l, etc)\n",
    "            field_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "            index: which entries in each field to read\n",
    "            epsg: an EPSG code specifying a projection (see www.epsg.org).  Good choices are:\n",
    "                for Greenland, 3413 (polar stereographic projection, with Greenland along the Y axis)\n",
    "                for Antarctica, 3031 (polar stereographic projection, centered on the Pouth Pole)\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a numpy array containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    if field_dict is None:\n",
    "        field_dict={None:['latitude','longitude','h_li', 'atl06_quality_summary'],\\\n",
    "                    'ground_track':['x_atc','y_atc'],\\\n",
    "                    'fit_statistics':['dh_fit_dx', 'dh_fit_dy']}\n",
    "    D={}\n",
    "    # below: file_re = regular expression, it will pull apart the regular expression to get the information from the filename\n",
    "    file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n",
    "    with h5py.File(filename,'r') as h5f:\n",
    "        for key in field_dict:\n",
    "            for ds in field_dict[key]:\n",
    "                if key is not None:\n",
    "                    ds_name=beam+'/land_ice_segments/'+key+'/'+ds\n",
    "                else:\n",
    "                    ds_name=beam+'/land_ice_segments/'+ds\n",
    "                if index is not None:\n",
    "                    D[ds]=np.array(h5f[ds_name][index])\n",
    "                else:\n",
    "                    D[ds]=np.array(h5f[ds_name])\n",
    "                if '_FillValue' in h5f[ds_name].attrs:\n",
    "                    bad_vals=D[ds]==h5f[ds_name].attrs['_FillValue']\n",
    "                    D[ds]=D[ds].astype(float)\n",
    "                    D[ds][bad_vals]=np.NaN\n",
    "        D['data_start_utc'] = h5f['/ancillary_data/data_start_utc'][:]\n",
    "        D['delta_time'] = h5f['/' + beam + '/land_ice_segments/delta_time'][:]\n",
    "        D['segment_id'] = h5f['/' + beam + '/land_ice_segments/segment_id'][:]\n",
    "    if epsg is not None:\n",
    "        xy=np.array(pyproj.proj.Proj(epsg)(D['longitude'], D['latitude']))\n",
    "        D['x']=xy[0,:].reshape(D['latitude'].shape)\n",
    "        D['y']=xy[1,:].reshape(D['latitude'].shape)\n",
    "    temp=file_re.search(filename)\n",
    "    D['rgt']=int(temp['rgt'])\n",
    "    D['cycle']=int(temp['cycle'])\n",
    "    D['beam']=beam\n",
    "    return D\n",
    "\n",
    "# A revised code to plot the elevations of segment midpoints (h_li):\n",
    "def plot_elevation(D6, ind=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot midpoint elevation for each ATL06 segment\n",
    "    \"\"\"\n",
    "    if ind is None:\n",
    "        ind=np.ones_like(D6['h_li'], dtype=bool)\n",
    "    # pull out heights of segment midpoints\n",
    "    h_li = D6['h_li'][ind]\n",
    "    # pull out along track x coordinates of segment midpoints\n",
    "    x_atc = D6['x_atc'][ind]\n",
    "\n",
    "    plt.plot(x_atc, h_li, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over rgts and do the correlation processing\n",
    "\n",
    "TOMORROW: START WITH NEXT CELL IN OLD CODE, IMPLEMENT MAKING THE X1 VEC AND LOOPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190523195046_08480311_003_01.h5']\n",
      "For cycle 03, read 1 data files of which 0 gave errors\n",
      "['/home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190822153035_08480411_003_01.h5']\n",
      "For cycle 04, read 1 data files of which 0 gave errors\n",
      "[]\n",
      "For cycle 05, read 0 data files of which 0 gave errors\n",
      "[]\n",
      "For cycle 06, read 0 data files of which 0 gave errors\n",
      "[]\n",
      "For cycle 07, read 0 data files of which 0 gave errors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-94eb53f381b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# cut out small chunk of data at time t1 (first cycle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mx_full_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_atc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcycle1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mix_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_full_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_full_t1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0mix_x2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mix_x1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_length\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mx_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_full_t1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix_x1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mix_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "# this could be future work\n",
    "\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "# try and smooth without filling nans\n",
    "dx = 20 # x_atc coordinate distance\n",
    "smoothing_window_size = int(np.round(40 / dx)) # meters / dx;\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset, because each point is 20 m apart\n",
    "filt = np.ones(smoothing_window_size)\n",
    "smoothed = True\n",
    "\n",
    "segment_length = 2000 # m\n",
    "search_width = 800 # m\n",
    "\n",
    "\n",
    "for rgt in rgts.keys():\n",
    "    if rgt == '0848': # just want to work on this track for now\n",
    "        \n",
    "        ### load all files for this rgt\n",
    "        rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "        \n",
    "        ### extract data from all available cycles\n",
    "        x_atc = {}\n",
    "        h_li_raw = {}\n",
    "        h_li = {}\n",
    "        h_li_diff = {}\n",
    "        times = {}\n",
    "        min_seg_ids = {}\n",
    "        min_x_atc = {}\n",
    "\n",
    "        cycles_this_rgt = []\n",
    "        for cycle in cycles:\n",
    "            # load data that matches cycle; put into dictionaries to use shortly\n",
    "            Di = {}\n",
    "            x_atc[cycle] = {}\n",
    "            h_li_raw[cycle] = {}\n",
    "            h_li[cycle] = {}\n",
    "            h_li_diff[cycle] = {}\n",
    "            times[cycle] = {}\n",
    "            min_seg_ids[cycle] = {}\n",
    "            min_x_atc[cycle] = {}\n",
    "\n",
    "\n",
    "\n",
    "            filenames = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}{cycle}*_003*.h5'))\n",
    "            print(filenames)\n",
    "            error_count=0\n",
    "            for filename in filenames:\n",
    "                try:\n",
    "                    for beam in beams:\n",
    "                        Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                        times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                        # extract h_li and x_atc for that section                \n",
    "                        x_atc_tmp = Di[filename]['x_atc']\n",
    "                        h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "\n",
    "                        # segment ids:\n",
    "                        seg_ids = Di[filename]['segment_id']\n",
    "                        min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                        #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                        # make a monotonically increasing x vector\n",
    "                        # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                        ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                        x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                        h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                        h_full[ind] = h_li_tmp\n",
    "                        min_x_atc[cycle][beam] = x_atc_tmp[0]\n",
    "\n",
    "\n",
    "                        x_atc[cycle][beam] = x_full\n",
    "                        h_li_raw[cycle][beam] = h_full\n",
    "\n",
    "                        # running average smoother /filter\n",
    "                        if smoothed == True:\n",
    "                            h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full, mode = 'same')\n",
    "                            h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                            # differentiate that section of data\n",
    "                            h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                        else: \n",
    "                            h_li[cycle][beam] = h_full\n",
    "                            h_diff = (h_full[1:] - h_full[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "\n",
    "                        h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "#                         # plot\n",
    "#                         axs[0].plot(x_full, h_full)\n",
    "#                         axs[1].plot(x_full[1:], h_diff)\n",
    "#         #                 axs[2].plot(x_atc_tmp[1:] - x_atc_tmp[:-1])\n",
    "#                         axs[2].plot(np.isnan(h_full))\n",
    "#                         axs[3].plot(seg_ids[1:]- seg_ids[:-1])\n",
    "\n",
    "\n",
    "                    cycles_this_rgt+=[cycle]\n",
    "\n",
    "\n",
    "                except KeyError as e:\n",
    "                    print(f'file {filename} encountered error {e}')\n",
    "                    error_count += 1\n",
    "            \n",
    "            print(f\"For cycle {cycle}, read {len(Di)} data files of which {error_count} gave errors\")\n",
    "            \n",
    "        ### Determine # of possible velocities:\n",
    "        n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later\n",
    "        for veloc_number in range(n_possible_veloc):\n",
    "            cycle1 = cycles[veloc_number]\n",
    "            cycle2 = cycles[veloc_number+1]\n",
    "            t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "            t1 = Time(t1_string)\n",
    "\n",
    "            t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "            t2 = Time(t2_string)\n",
    "\n",
    "            dt = (t2 - t1).jd # difference in julian days\n",
    "        \n",
    "            velocities = {}     \n",
    "            for beam in beams:\n",
    "                # fig1, axs = plt.subplots(4,1)\n",
    "\n",
    "                ### determine x1: larger value for both beams, if different\n",
    "                x1 = np.nanmax([x_atc[cycle1][beam][0], x_atc[cycle2][beam][0]])                \n",
    "                \n",
    "                # cut out small chunk of data at time t1 (first cycle)\n",
    "                x_full_t1 = x_atc[cycle1][beam]\n",
    "                ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0]\n",
    "                ix_x2 = ix_x1 + int(np.round(segment_length/dx))      \n",
    "                x_t1 = x_full_t1[ix_x1:ix_x2]\n",
    "                h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # start 1 index earlier because the data are differentiated\n",
    "\n",
    "                # cut out a wider chunk of data at time t2 (second cycle)\n",
    "                x_full_t2 = x_atc[cycle2][beam]\n",
    "                ix_x3 = ix_x1 - int(np.round(search_width/dx)) # offset on earlier end by # indices in search_width\n",
    "                ix_x4 = ix_x2 + int(np.round(search_width/dx)) # offset on later end by # indices in search_width\n",
    "                x_t2 = x_full_t2[ix_x3:ix_x4]\n",
    "                h_li2 = h_li_diff[cycle2][beam][ix_x3:ix_x4]\n",
    "\n",
    "                # plot data\n",
    "                # axs[0].plot(x_t2, h_li2, 'r')\n",
    "                # axs[0].plot(x_t1, h_li1, 'k')\n",
    "                # axs[0].set_xlabel('x_atc (m)')\n",
    "\n",
    "                # correlate old with newer data\n",
    "                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                # normalize correlation function; simplest way (not quite correct)\n",
    "                # norm_val = np.sqrt(np.sum(h_li1**2)*np.sum(h_li2**2)) # normalize so values range between 0 and 1\n",
    "                # corr = corr / norm_val\n",
    "\n",
    "                # a better way to normalize correlation function: shifting along longer vector\n",
    "                coeff_a_val = np.sum(h_li1**2)\n",
    "                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                    coeff_b_val[shift] = np.sum(h_li2[shift:shift + len(h_li1)]**2)\n",
    "                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                corr = corr / norm_vec\n",
    "\n",
    "\n",
    "        #         lagvec = np.arange( -(len(h_li1) - 1), len(h_li2), 1)# for mode = 'full'\n",
    "        #         lagvec = np.arange( -int(search_width/dx) - 1, int(search_width/dx) +1, 1) # for mode = 'valid'\n",
    "                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                shift_vec = lagvec * dx\n",
    "\n",
    "                ix_peak = np.arange(len(corr))[corr == np.nanmax(corr)][0]\n",
    "                best_lag = lagvec[ix_peak]\n",
    "                best_shift = shift_vec[ix_peak]\n",
    "                velocities[beam] = best_shift/(dt/365)\n",
    "\n",
    "                # axs[1].plot(lagvec,corr)\n",
    "                # axs[1].plot(lagvec[ix_peak],corr[ix_peak], 'r*')\n",
    "                # axs[1].set_xlabel('lag (samples)')\n",
    "\n",
    "                # axs[2].plot(shift_vec,corr)\n",
    "                # axs[2].plot(shift_vec[ix_peak],corr[ix_peak], 'r*')\n",
    "                # axs[2].set_xlabel('shift (m)')\n",
    "\n",
    "                ## plot shifted data\n",
    "                # axs[3].plot(x_t2, h_li2, 'r')\n",
    "                # axs[3].plot(x_t1 - best_shift, h_li1, 'k')\n",
    "                # axs[3].set_xlabel('x_atc (m)')\n",
    "\n",
    "                # axs[0].text(x_t2[100], 0.6*np.nanmax(h_li2), beam)\n",
    "                # axs[1].text(lagvec[5], 0.6*np.nanmax(corr), 'best lag: ' + str(best_lag) + '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                # axs[2].text(shift_vec[5], 0.6*np.nanmax(corr), 'best shift: ' + str(best_shift) + ' m'+ '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                # axs[2].text(shift_vec[5], 0.3*np.nanmax(corr), 'veloc of ' + str(np.round(best_shift/(dt/365),1)) + ' m/yr')\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "#         Di={}\n",
    "#         error_count=0\n",
    "#         for file in rgt_files:\n",
    "#             try:\n",
    "#                 D_dict[file]=atl06_to_dict(file, '/gt2l', index=slice(0, -1, 25), epsg=3031)\n",
    "#             except KeyError as e:\n",
    "#                 print(f'file {file} encountered error {e}')\n",
    "#                 error_count += 1\n",
    "#         print(f\"read {len(D_dict)} data files of which {error_count} gave errors\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03', '04']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycles_this_rgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'03': {'gt1l': 29123498.50436068,\n",
       "  'gt1r': 29123518.40319396,\n",
       "  'gt2l': 29124294.457597345,\n",
       "  'gt2r': 29124314.356425773,\n",
       "  'gt3l': 29125110.309463445,\n",
       "  'gt3r': 29125130.2082869},\n",
       " '04': {'gt1l': 29123498.50436068,\n",
       "  'gt1r': 29123518.40319396,\n",
       "  'gt2l': 29124294.457597345,\n",
       "  'gt2r': 29124314.356425773,\n",
       "  'gt3l': 29125090.410639867,\n",
       "  'gt3r': 29125130.2082869},\n",
       " '05': {},\n",
       " '06': {},\n",
       " '07': {}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x_atc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
