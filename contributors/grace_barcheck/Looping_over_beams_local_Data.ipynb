{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Query icepyx; see what tracks are available in area of interest <-- doesn't work, using local data instead\n",
    "\n",
    "2. Save track numbers, beams, and repeat numbers into a dictionary\n",
    "\n",
    "3. For each track/beam combination, loop over all possible repeat pairs\n",
    "\n",
    "    A. Load all beams and all repeats for that track using icepyx (?). For all beams / repeats:\n",
    "    \n",
    "        - Do whatever we are doing with ATL03\n",
    "    \n",
    "        - Fill in nan gaps with noise\n",
    "        \n",
    "    B. For each repeat pair:\n",
    "        \n",
    "        - Loop across the along track coordinates: \n",
    "        \n",
    "            Choices: window size, search width, running average window size, step, where to save data geographically\n",
    "            \n",
    "            Output: Best lag, corresponding correlation coefficient, equivalent along-track velocity\n",
    "            \n",
    "        - Save results in a text file with date collected, dx from ATL03 processing, lat, lon, veloc, correlation coefficient, best lag, # contributing nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepyx import icesat2data as ipd\n",
    "import os, glob, re, h5py, sys, pyproj\n",
    "import matplotlib as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from astropy.time import Time\n",
    "from scipy.signal import correlate, detrend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "ATL06_files=glob.glob(os.path.join(datapath, '*.h5'))\n",
    "\n",
    "out_path = 'shared/surface_velocity/ATL06_out/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shared/surface_velocity/ATL06_out/\n"
     ]
    }
   ],
   "source": [
    "#scratch cell\n",
    "out_path = '/home/jovyan/shared/surface_velocity/ATL06_out/'\n",
    "file_out = f'{out_path}'\n",
    "\n",
    "print(file_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0080', '1131', '0232', '1031', '0634', '0507', '0131', '0192', '0354', '1061', '0492', '0690', '0970', '0187', '0558', '1335', '0741', '0659', '0894', '1183', '0680', '1101', '1168', '0034', '0568', '0705', '0293', '0711', '1040', '0070', '0543', '1244', '1192', '0314', '0126', '1193', '1147', '0253', '0451', '1122', '0994', '0391', '0141', '0979', '0476', '1223', '1137', '0726', '0918', '1314', '1253', '1177', '0750', '0330', '1010', '0193', '0781', '0872', '1299', '0629', '1055', '0695', '0309', '0467', '0802', '0644', '0461', '0415', '0635', '0924', '0482', '1214', '1076', '0573', '0339', '0833', '0171', '0446', '0385', '1336', '0796', '0369', '0756', '1238', '0674', '0903', '0955', '0650', '0772', '0832', '0766', '0513', '0308', '0857', '0720', '1162', '0848', '0202', '0019', '0071', '1138', '1259', '0522', '0390', '1254', '0360', '0933', '1025', '0512', '1000', '1153', '0842', '0400', '1351', '0751', '0628', '0537', '0583', '0878', '1320', '0491', '0552', '0421', '1315', '1015', '0954', '0040', '1275', '0132', '0812', '0598', '0985', '1016', '1330', '0324', '0589', '1132', '0909', '0049', '0370', '0263', '0735', '0613', '0095', '0431', '0345', '1071', '0689', '1274', '1376', '1345', '0811', '0004', '0888', '0452', '0065', '0964', '0217', '1208', '0893', '0147', '1360', '0939', '1092', '1229', '0110', '1375', '0406', '0817', '0436', '0863', '0787', '0934', '0574', '0949', '0873', '1070', '0497', '0009', '1366', '0528', '1198', '0025', '0619', '0247', '0010', '1107', '1046', '0162', '1305', '1116', '0604', '0827', '0665', '0430', '0116', '0771', '0208', '0086', '0375', '0186', '0248', '0299', '1381', '0284', '0269', '1086', '1290', '0223', '1284', '0177', '0101', '1077', '0156', '0278', '1269', '0238', '0055'])\n",
      "['04', '02', '03', '01']\n"
     ]
    }
   ],
   "source": [
    "rgts = {}\n",
    "for filepath in ATL06_files:\n",
    "    filename = filepath.split('/')[-1]\n",
    "    rgt = filename.split('_')[3][0:4]\n",
    "    track = filename.split('_')[3][4:6]\n",
    "#     print(rgt,track)\n",
    "    if not rgt in rgts.keys():\n",
    "        rgts[rgt] = []\n",
    "        rgts[rgt].append(track)\n",
    "    else:\n",
    "        rgts[rgt].append(track)\n",
    "\n",
    "\n",
    "# all rgt values in our study are are in rgts.keys()\n",
    "print(rgts.keys())\n",
    "\n",
    "# available tracks for each rgt are in rgts[rgt]; ex.:\n",
    "print(rgts['0848'])\n",
    "\n",
    "# let's work 0848, our first good track friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atl06_to_dict(filename, beam, field_dict=None, index=None, epsg=None):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            beam: a string specifying which beam is to be read (ex: gt1l, gt1r, gt2l, etc)\n",
    "            field_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "            index: which entries in each field to read\n",
    "            epsg: an EPSG code specifying a projection (see www.epsg.org).  Good choices are:\n",
    "                for Greenland, 3413 (polar stereographic projection, with Greenland along the Y axis)\n",
    "                for Antarctica, 3031 (polar stereographic projection, centered on the Pouth Pole)\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a numpy array containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    if field_dict is None:\n",
    "        field_dict={None:['latitude','longitude','h_li', 'atl06_quality_summary'],\\\n",
    "                    'ground_track':['x_atc','y_atc'],\\\n",
    "                    'fit_statistics':['dh_fit_dx', 'dh_fit_dy']}\n",
    "    D={}\n",
    "    # below: file_re = regular expression, it will pull apart the regular expression to get the information from the filename\n",
    "    file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n",
    "    with h5py.File(filename,'r') as h5f:\n",
    "        for key in field_dict:\n",
    "            for ds in field_dict[key]:\n",
    "                if key is not None:\n",
    "                    ds_name=beam+'/land_ice_segments/'+key+'/'+ds\n",
    "                else:\n",
    "                    ds_name=beam+'/land_ice_segments/'+ds\n",
    "                if index is not None:\n",
    "                    D[ds]=np.array(h5f[ds_name][index])\n",
    "                else:\n",
    "                    D[ds]=np.array(h5f[ds_name])\n",
    "                if '_FillValue' in h5f[ds_name].attrs:\n",
    "                    bad_vals=D[ds]==h5f[ds_name].attrs['_FillValue']\n",
    "                    D[ds]=D[ds].astype(float)\n",
    "                    D[ds][bad_vals]=np.NaN\n",
    "        D['data_start_utc'] = h5f['/ancillary_data/data_start_utc'][:]\n",
    "        D['delta_time'] = h5f['/' + beam + '/land_ice_segments/delta_time'][:]\n",
    "        D['segment_id'] = h5f['/' + beam + '/land_ice_segments/segment_id'][:]\n",
    "    if epsg is not None:\n",
    "        xy=np.array(pyproj.proj.Proj(epsg)(D['longitude'], D['latitude']))\n",
    "        D['x']=xy[0,:].reshape(D['latitude'].shape)\n",
    "        D['y']=xy[1,:].reshape(D['latitude'].shape)\n",
    "    temp=file_re.search(filename)\n",
    "    D['rgt']=int(temp['rgt'])\n",
    "    D['cycle']=int(temp['cycle'])\n",
    "    D['beam']=beam\n",
    "    return D\n",
    "\n",
    "# A revised code to plot the elevations of segment midpoints (h_li):\n",
    "def plot_elevation(D6, ind=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot midpoint elevation for each ATL06 segment\n",
    "    \"\"\"\n",
    "    if ind is None:\n",
    "        ind=np.ones_like(D6['h_li'], dtype=bool)\n",
    "    # pull out heights of segment midpoints\n",
    "    h_li = D6['h_li'][ind]\n",
    "    # pull out along track x coordinates of segment midpoints\n",
    "    x_atc = D6['x_atc'][ind]\n",
    "\n",
    "    plt.plot(x_atc, h_li, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over rgts and do the correlation processing\n",
    "\n",
    "TOMORROW: START WITH NEXT CELL IN OLD CODE, IMPLEMENT MAKING THE X1 VEC AND LOOPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, path_to_data, product):\n",
    "    \"\"\" \n",
    "    rgt: repeat ground track number of desired data\n",
    "    smoothing: if true, a centered running avergae filter of smoothing_window_size will be used\n",
    "    smoothing_window_size: how large a smoothing window to use (in meters)\n",
    "    dx: desired spacing \n",
    "    path_to_data: \n",
    "    product: ex., ATL06\n",
    "    \"\"\" \n",
    "    \n",
    "    # hard code these for now:\n",
    "    cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "    beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r'] \n",
    "\n",
    "    ### extract data from all available cycles\n",
    "    x_atc = {}\n",
    "    lats = {}\n",
    "    lons = {}\n",
    "    h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "    h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "    h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "    h_li_diff = {}\n",
    "    times = {}\n",
    "    min_seg_ids = {}\n",
    "    segment_ids = {}\n",
    "\n",
    "    cycles_this_rgt = []\n",
    "    for cycle in cycles: # loop over all available cycles\n",
    "        Di = {}\n",
    "        x_atc[cycle] = {}\n",
    "        lats[cycle] = {}\n",
    "        lons[cycle] = {}\n",
    "        h_li_raw[cycle] = {}\n",
    "        h_li_raw_NoNans[cycle] = {}\n",
    "        h_li[cycle] = {}\n",
    "        h_li_diff[cycle] = {}\n",
    "        times[cycle] = {}\n",
    "        min_seg_ids[cycle] = {}\n",
    "        segment_ids[cycle] = {}\n",
    "\n",
    "\n",
    "        filenames = glob.glob(os.path.join(path_to_data, f'*{product}_*_{rgt}{cycle}*_003*.h5'))\n",
    "        error_count=0\n",
    "\n",
    "\n",
    "        for filename in filenames: # try and load any available files; hopefully is just one\n",
    "            try:\n",
    "                for beam in beams:\n",
    "                    Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                    times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                    # extract h_li and x_atc, and lat/lons for that section                \n",
    "                    x_atc_tmp = Di[filename]['x_atc']\n",
    "                    h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                    lats_tmp = Di[filename]['latitude']\n",
    "                    lons_tmp = Di[filename]['longitude']\n",
    "\n",
    "\n",
    "                    # segment ids:\n",
    "                    seg_ids = Di[filename]['segment_id']\n",
    "                    min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                    #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                    # make a monotonically increasing x vector\n",
    "                    # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                    ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                    x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                    h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    h_full[ind] = h_li_tmp\n",
    "                    lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lats_full[ind] = lats_tmp\n",
    "                    lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lons_full[ind] = lons_tmp\n",
    "\n",
    "                    ## save the segment id's themselves, with gaps filled in\n",
    "                    segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "\n",
    "                    x_atc[cycle][beam] = x_full\n",
    "                    h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                    lons[cycle][beam] = lons_full\n",
    "                    lats[cycle][beam] = lats_full\n",
    "\n",
    "                    ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                    ### interpolate nans in pandas\n",
    "                    # put in dataframe for just this step; eventually rewrite to use only dataframes?              \n",
    "                    data = {'x_full': x_full, 'h_full': h_full}\n",
    "                    df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                    #df.plot(x='x_full',y='h_full')\n",
    "                    # linear interpolation for now\n",
    "                    df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                    h_full_interp = df['h_full'].values\n",
    "                    h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                    # running average smoother /filter\n",
    "                    if smoothing == True:\n",
    "                        h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                        h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                        # differentiate that section of data\n",
    "                        h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    else: \n",
    "                        h_li[cycle][beam] = h_full_interp\n",
    "                        h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "\n",
    "\n",
    "                    print(len(x_full), len(h_full), len(lats_full), len(seg_ids), len(h_full_interp), len(h_diff))\n",
    "\n",
    "\n",
    "                cycles_this_rgt+=[cycle]\n",
    "            except KeyError as e:\n",
    "                print(f'file {filename} encountered error {e}')\n",
    "                error_count += 1\n",
    "\n",
    "    print(cycles_this_rgt)\n",
    "    return x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, \\\n",
    "            times, min_seg_ids, segment_ids, cycles_this_rgt\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0080, #0 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1131, #1 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0232, #2 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1031, #3 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0634, #4 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "2588 2588 2588 2576 2588 2587\n",
      "2601 2601 2601 2589 2601 2600\n",
      "3023 3023 3023 2927 3023 3022\n",
      "3034 3034 3034 2938 3034 3033\n",
      "3458 3458 3458 3417 3458 3457\n",
      "3470 3470 3470 3429 3470 3469\n",
      "2588 2588 2588 2588 2588 2587\n",
      "2600 2600 2600 2600 2600 2599\n",
      "3022 3022 3022 3022 3022 3021\n",
      "3033 3033 3033 3033 3033 3032\n",
      "3457 3457 3457 3457 3457 3456\n",
      "3469 3469 3469 3469 3469 3468\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 0507, #5 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "4516 4516 4516 2335 4516 4515\n",
      "4530 4530 4530 2349 4530 4529\n",
      "4979 4979 4979 2410 4979 4978\n",
      "4992 4992 4992 2423 4992 4991\n",
      "5444 5444 5444 2637 5444 5443\n",
      "5457 5457 5457 2650 5457 5456\n",
      "4517 4517 4517 4517 4517 4516\n",
      "4530 4530 4530 4530 4530 4529\n",
      "4980 4980 4980 4980 4980 4979\n",
      "4993 4993 4993 4993 4993 4992\n",
      "5444 5444 5444 5444 5444 5443\n",
      "5458 5458 5458 5458 5458 5457\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 0131, #6 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0192, #7 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0354, #8 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1061, #9 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0492, #10 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190430122344_04920311_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190730080323_04920411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "[]\n",
      "\n",
      "Processing rgt 0690, #11 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "2153 2153 2153 2153 2153 2152\n",
      "2163 2163 2163 2163 2163 2162\n",
      "2580 2580 2580 2580 2580 2579\n",
      "2593 2593 2593 2593 2593 2592\n",
      "3010 3010 3010 3010 3010 3009\n",
      "3023 3023 3023 3023 3023 3022\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190812071246_06900411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "['03']\n",
      "\n",
      "Processing rgt 0970, #12 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0187, #13 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0558, #14 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "8291 8291 8291 8290 8291 8290\n",
      "8305 8305 8305 8304 8305 8304\n",
      "8809 8809 8809 8809 8809 8808\n",
      "8824 8824 8824 8824 8824 8823\n",
      "9328 9328 9328 9328 9328 9327\n",
      "9343 9343 9343 9343 9343 9342\n",
      "8288 8288 8288 8087 8288 8287\n",
      "8303 8303 8303 8102 8303 8302\n",
      "8806 8806 8806 8321 8806 8805\n",
      "8820 8820 8820 8335 8820 8819\n",
      "9326 9326 9326 8904 9326 9325\n",
      "9340 9340 9340 8918 9340 9339\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 1335, #15 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0741, #16 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "11718 11718 11718 11717 11718 11717\n",
      "11722 11722 11722 11721 11722 11721\n",
      "11889 11889 11889 11837 11889 11888\n",
      "11895 11895 11895 11843 11895 11894\n",
      "12062 12062 12062 11980 12062 12061\n",
      "12067 12067 12067 11985 12067 12066\n",
      "11717 11717 11717 11717 11717 11716\n",
      "11723 11723 11723 11723 11723 11722\n",
      "11890 11890 11890 11890 11890 11889\n",
      "11895 11895 11895 11895 11895 11894\n",
      "12062 12062 12062 12062 12062 12061\n",
      "12066 12066 12066 12066 12066 12065\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 0659, #17 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "17385 17385 17385 17385 17385 17384\n",
      "17393 17393 17393 17393 17393 17392\n",
      "17637 17637 17637 17637 17637 17636\n",
      "17643 17643 17643 17643 17643 17642\n",
      "17886 17886 17886 17864 17886 17885\n",
      "17894 17894 17894 17872 17894 17893\n",
      "17386 17386 17386 17386 17386 17385\n",
      "17393 17393 17393 17393 17393 17392\n",
      "17636 17636 17636 17636 17636 17635\n",
      "17644 17644 17644 17644 17644 17643\n",
      "17887 17887 17887 17866 17887 17886\n",
      "17894 17894 17894 17873 17894 17893\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 0894, #18 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "10050 10050 10050 10050 10050 10049\n",
      "10033 10033 10033 10033 10033 10032\n",
      "9425 9425 9425 9425 9425 9424\n",
      "9407 9407 9407 9407 9407 9406\n",
      "8799 8799 8799 8799 8799 8798\n",
      "8781 8781 8781 8781 8781 8780\n",
      "10053 10053 10053 10014 10053 10052\n",
      "10035 10035 10035 9996 10035 10034\n",
      "9427 9427 9427 9401 9427 9426\n",
      "9409 9409 9409 9383 9409 9408\n",
      "8802 8802 8802 8627 8802 8801\n",
      "8783 8783 8783 8608 8783 8782\n",
      "['03', '04']\n",
      "\n",
      "Processing rgt 1183, #19 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0680, #20 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "10617 10617 10617 10617 10617 10616\n",
      "10632 10632 10632 10632 10632 10631\n",
      "11168 11168 11168 11168 11168 11167\n",
      "11184 11184 11184 11184 11184 11183\n",
      "11364 11364 11364 11364 11364 11363\n",
      "11369 11369 11369 11369 11369 11368\n",
      "10615 10615 10615 10615 10615 10614\n",
      "10630 10630 10630 10630 10630 10629\n",
      "11166 11166 11166 11166 11166 11165\n",
      "11182 11182 11182 11182 11182 11181\n",
      "11362 11362 11362 11361 11362 11361\n",
      "11367 11367 11367 11366 11367 11366\n",
      "['03', '04']\n",
      "Total number of repeat tracks successfully processed = 9\n"
     ]
    }
   ],
   "source": [
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "# this could be future work\n",
    "\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "product = 'ATL06'\n",
    "dx = 20 # x_atc coordinate distance\n",
    "\n",
    "# control \n",
    "segment_length = 2000 # m\n",
    "search_width = 800 # m\n",
    "along_track_step = 100 # m; how much to jump between each veloc determination\n",
    "max_percent_nans = 10 # what % of segment length can be nans\n",
    "\n",
    "# smoothing\n",
    "smoothing = True\n",
    "smoothing_window_size = int(np.round(40 / dx)) # meters / dx;\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset, because each point is 20 m apart\n",
    "filt = np.ones(smoothing_window_size)\n",
    "\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir <= 20: # just process a few for the moment\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### load all files for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "                \n",
    "                ### extract data from all available cycles\n",
    "                x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, times, min_seg_ids, segment_ids, cycles_this_rgt = \\\n",
    "                    load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, datapath, product)\n",
    "                # 98% sure this code returns the correct values\n",
    "                \n",
    "                ### Determine # of possible velocities:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    for beam in beams:\n",
    "                        # fig1, axs = plt.subplots(4,1)\n",
    "\n",
    "\n",
    "                        ### determine x1: larger value for both beams, if different\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            n_segments_this_track = (len(x_atc[cycletmp][beam]) - search_width/dx) / (along_track_step/dx)\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "                        ### determine xend: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= smallest_xatc - search_width/dx)\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### dicts to store info in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                        \n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            # cut out small chunk of data at time t1 (first cycle)\n",
    "                            x_full_t1 = x_atc[cycle1][beam]\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0]\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx))      \n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2]\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2]\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2]\n",
    "                            seg_ids_t1 = segment_ids[cycle1][beam][ix_x1:ix_x2]\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # find midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            # cut out a wider chunk of data at time t2 (second cycle)\n",
    "                            x_full_t2 = x_atc[cycle2][beam]\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # offset on earlier end by # indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # offset on later end by # indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4]\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # plot data\n",
    "                            # axs[0].plot(x_t2, h_li2, 'r')\n",
    "                            # axs[0].plot(x_t1, h_li1, 'k')\n",
    "                            # axs[0].set_xlabel('x_atc (m)')\n",
    "\n",
    "                            ### if there are fewer than 10% nans in either data chunk:\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # correlate old with newer data\n",
    "                                # detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # normalize both chunks of data\n",
    "            #                         h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "            #                         h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                # a better way to normalize correlation function: shifting along longer vector\n",
    "                                # normalize by autocorrelations\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but it does\n",
    "\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                                \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        h5_file_out = f'{out_path}rgt{rgt}.hdf5'\n",
    "                        with h5py.File(h5_file_out, 'w') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "\n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "            with h5py.File(h5_file_out, 'w') as f:\n",
    "                f['dx'] = dx \n",
    "                f['product'] = product \n",
    "                f['segment_length'] = segment_length \n",
    "                f['search_width'] = search_width \n",
    "                f['along_track_step'] = along_track_step \n",
    "                f['max_percent_nans'] = max_percent_nans \n",
    "                f['smoothing'] = smoothing \n",
    "                f['smoothing_window_size'] = smoothing_window_size \n",
    "                f['process_date'] = str(Time.now().value) \n",
    "\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            print(e)\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 13:07:17.512208\n"
     ]
    }
   ],
   "source": [
    "tt = Time.now().value\n",
    "str(tt)\n",
    "print(str(Time.now().value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0894, #18 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "rgt 0894 encountered an error\n",
      "Total number of repeat tracks successfully processed = 0\n"
     ]
    }
   ],
   "source": [
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "# this could be future work\n",
    "\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "# try and smooth without filling nans\n",
    "dx = 20 # x_atc coordinate distance\n",
    "smoothing_window_size = int(np.round(40 / dx)) # meters / dx;\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset, because each point is 20 m apart\n",
    "filt = np.ones(smoothing_window_size)\n",
    "smoothed = True\n",
    "\n",
    "segment_length = 2000 # m\n",
    "search_width = 800 # m\n",
    "\n",
    "along_track_step = 100 # m; how much to jump between each veloc determination\n",
    "\n",
    "max_percent_nans = 10 # what % of segment length can be nans\n",
    "\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir == 18: # just process a few for the moment\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### load all files for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "                ### extract data from all available cycles\n",
    "                x_atc = {}\n",
    "                lats = {}\n",
    "                lons = {}\n",
    "                h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "                h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "                h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "                h_li_diff = {}\n",
    "                times = {}\n",
    "                min_seg_ids = {}\n",
    "                segment_ids = {}\n",
    "\n",
    "\n",
    "                cycles_this_rgt = []\n",
    "                for cycle in cycles:\n",
    "                    # load data that matches cycle; put into dictionaries to use shortly\n",
    "                    Di = {}\n",
    "                    x_atc[cycle] = {}\n",
    "                    lats[cycle] = {}\n",
    "                    lons[cycle] = {}\n",
    "                    h_li_raw[cycle] = {}\n",
    "                    h_li_raw_NoNans[cycle] = {}\n",
    "                    h_li[cycle] = {}\n",
    "                    h_li_diff[cycle] = {}\n",
    "                    times[cycle] = {}\n",
    "                    min_seg_ids[cycle] = {}\n",
    "                    segment_ids[cycle] = {}\n",
    "\n",
    "                    filenames = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}{cycle}*_003*.h5'))\n",
    "                    #print(filenames)\n",
    "                    error_count=0\n",
    "                    for filename in filenames:\n",
    "                        try:\n",
    "                            for beam in beams:\n",
    "                                Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                                times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                                # extract h_li and x_atc, and lat/lons for that section                \n",
    "                                x_atc_tmp = Di[filename]['x_atc']\n",
    "                                h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                                lats_tmp = Di[filename]['latitude']\n",
    "                                lons_tmp = Di[filename]['longitude']\n",
    "\n",
    "\n",
    "                                # segment ids:\n",
    "                                seg_ids = Di[filename]['segment_id']\n",
    "                                min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                                #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                                # make a monotonically increasing x vector\n",
    "                                # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                                ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                                x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                                h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                                h_full[ind] = h_li_tmp\n",
    "                                lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                                lats_full[ind] = lats_tmp\n",
    "                                lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                                lons_full[ind] = lons_tmp\n",
    "                                \n",
    "                                ## save the segment id's themselves, with gaps filled in\n",
    "                                segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                                segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "                                \n",
    "                                x_atc[cycle][beam] = x_full\n",
    "                                h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                                lons[cycle][beam] = lons_full\n",
    "                                lats[cycle][beam] = lats_full\n",
    "\n",
    "                                ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                                ### interpolate nans in pandas\n",
    "                                # put in dataframe for just this step; eventually rewrite to use only dataframes?\n",
    "                                data = {'x_full': x_full, 'h_full': h_full}\n",
    "                                df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                                #df.plot(x='x_full',y='h_full')\n",
    "                                # linear interpolation for now\n",
    "                                df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                                h_full_interp = df['h_full'].values\n",
    "                                h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                                # running average smoother /filter\n",
    "                                if smoothed == True:\n",
    "                                    h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                                    h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                                    # differentiate that section of data\n",
    "                                    h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                                else: \n",
    "                                    h_li[cycle][beam] = h_full_interp\n",
    "                                    h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                                h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "                            cycles_this_rgt+=[cycle]\n",
    "\n",
    "\n",
    "                        except KeyError as e:\n",
    "                            print(f'file {filename} encountered error {e}')\n",
    "                            error_count += 1\n",
    "\n",
    "                    #print(f\"For rgt {rgt} cycle {cycle}, read {len(Di)} data files of which {error_count} gave errors\")\n",
    "\n",
    "\n",
    "                ### Determine # of possible velocities:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    for beam in beams:\n",
    "                        # fig1, axs = plt.subplots(4,1)\n",
    "\n",
    "\n",
    "                        ### determine x1: larger value for both beams, if different\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            n_segments_this_track = (len(x_atc[cycletmp][beam]) - search_width/dx) / (along_track_step/dx)\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "                        ### determine xend: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= smallest_xatc - search_width/dx)\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### dicts to store info in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                        \n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            # cut out small chunk of data at time t1 (first cycle)\n",
    "                            x_full_t1 = x_atc[cycle1][beam]\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0]\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx))      \n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2]\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2]\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2]\n",
    "                            seg_ids_t1 = seg_ids[cycle1][beam][ix_x1:ix_x2]\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # find midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            # cut out a wider chunk of data at time t2 (second cycle)\n",
    "                            x_full_t2 = x_atc[cycle2][beam]\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # offset on earlier end by # indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # offset on later end by # indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4]\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # plot data\n",
    "                            # axs[0].plot(x_t2, h_li2, 'r')\n",
    "                            # axs[0].plot(x_t1, h_li1, 'k')\n",
    "                            # axs[0].set_xlabel('x_atc (m)')\n",
    "\n",
    "                            ### if there are fewer than 10% nans in either data chunk:\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # correlate old with newer data\n",
    "                                # detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # normalize both chunks of data\n",
    "            #                         h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "            #                         h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                # a better way to normalize correlation function: shifting along longer vector\n",
    "                                # normalize by autocorrelations\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but it does\n",
    "\n",
    "\n",
    "                        #         lagvec = np.arange( -(len(h_li1) - 1), len(h_li2), 1)# for mode = 'full'\n",
    "                        #         lagvec = np.arange( -int(search_width/dx) - 1, int(search_width/dx) +1, 1) # for mode = 'valid'\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                                \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        h5_file_out = f'{out_path}rgt{rgt}.hdf5'\n",
    "                        with h5py.File(h5_file_out, 'w') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            \n",
    "#                         f'{out_path}rgt{rgt}_{beam}.txt'\n",
    "#                         f = open(file_out,'w')\n",
    "\n",
    "\n",
    "#                         header0 = 'segment_length='+str(segment_length)+',segment_step='+str((dx))+'m,search_width='+str(search_width) + 'm'\n",
    "#                         header = 'x_atc_segment_middle'\n",
    "#                         for beam in beams:\n",
    "#                             header = header + ',' + beam + '_veloc,' + beam + '_correlationValue'\n",
    "#                         f.write(header0 + '\\n')\n",
    "#                         f.write(header + '\\n')\n",
    "                \n",
    "                \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "        except (ValueError, IndexError):\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "                    # axs[1].plot(lagvec,corr)\n",
    "                    # axs[1].plot(lagvec[ix_peak],corr[ix_peak], 'r*')\n",
    "                    # axs[1].set_xlabel('lag (samples)')\n",
    "\n",
    "                    # axs[2].plot(shift_vec,corr)\n",
    "                    # axs[2].plot(shift_vec[ix_peak],corr[ix_peak], 'r*')\n",
    "                    # axs[2].set_xlabel('shift (m)')\n",
    "\n",
    "                    ## plot shifted data\n",
    "                    # axs[3].plot(x_t2, h_li2, 'r')\n",
    "                    # axs[3].plot(x_t1 - best_shift, h_li1, 'k')\n",
    "                    # axs[3].set_xlabel('x_atc (m)')\n",
    "\n",
    "                    # axs[0].text(x_t2[100], 0.6*np.nanmax(h_li2), beam)\n",
    "                    # axs[1].text(lagvec[5], 0.6*np.nanmax(corr), 'best lag: ' + str(best_lag) + '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                    # axs[2].text(shift_vec[5], 0.6*np.nanmax(corr), 'best shift: ' + str(best_shift) + ' m'+ '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                    # axs[2].text(shift_vec[5], 0.3*np.nanmax(corr), 'veloc of ' + str(np.round(best_shift/(dt/365),1)) + ' m/yr')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7a7cf9dc784f349faab5458f448fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'0558'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7572b536c07f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for ir, rgt in enumerate(rgts.keys()):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     if ir <= 10:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0558'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '0558'"
     ]
    }
   ],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(velocities[beam])\n",
    "# tmp = ma.array(velocities[beam], mask = correlations[beam] > 0.8)\n",
    "plt.figure()\n",
    "# for ir, rgt in enumerate(rgts.keys()):\n",
    "#     if ir <= 10:\n",
    "plt.plot(correlations['0558'][beam],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
