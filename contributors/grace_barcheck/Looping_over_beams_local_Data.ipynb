{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepyx import icesat2data as ipd\n",
    "import os, glob, re, h5py, sys, pyproj\n",
    "import matplotlib as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from astropy.time import Time\n",
    "from scipy.signal import correlate, detrend\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import pointCollection as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Where are the data to be processed\n",
    "datapath = '/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "ATL06_files=glob.glob(os.path.join(datapath, '*.h5'))\n",
    "\n",
    "\n",
    "### Where to save the results\n",
    "out_path = '/home/jovyan/shared/surface_velocity/ATL06_out/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a list of all available repeat ground tracks in the folder with data in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0080', '1131', '0232', '1031', '0634', '0507', '0131', '0192', '0354', '1061', '0492', '0690', '0970', '0187', '0558', '1335', '0741', '0659', '0894', '1183', '0680', '1101', '1168', '0034', '0568', '0705', '0293', '0711', '1040', '0070', '0543', '1244', '1192', '0314', '0126', '1193', '1147', '0253', '0451', '1122', '0994', '0391', '0141', '0979', '0476', '1223', '1137', '0726', '0918', '1314', '1253', '1177', '0750', '0330', '1010', '0193', '0781', '0872', '1299', '0629', '1055', '0695', '0309', '0467', '0802', '0644', '0461', '0415', '0635', '0924', '0482', '1214', '1076', '0573', '0339', '0833', '0171', '0446', '0385', '1336', '0796', '0369', '0756', '1238', '0674', '0903', '0955', '0650', '0772', '0832', '0766', '0513', '0308', '0857', '0720', '1162', '0848', '0202', '0019', '0071', '1138', '1259', '0522', '0390', '1254', '0360', '0933', '1025', '0512', '1000', '1153', '0842', '0400', '1351', '0751', '0628', '0537', '0583', '0878', '1320', '0491', '0552', '0421', '1315', '1015', '0954', '0040', '1275', '0132', '0812', '0598', '0985', '1016', '1330', '0324', '0589', '1132', '0909', '0049', '0370', '0263', '0735', '0613', '0095', '0431', '0345', '1071', '0689', '1274', '1376', '1345', '0811', '0004', '0888', '0452', '0065', '0964', '0217', '1208', '0893', '0147', '1360', '0939', '1092', '1229', '0110', '1375', '0406', '0817', '0436', '0863', '0787', '0934', '0574', '0949', '0873', '1070', '0497', '0009', '1366', '0528', '1198', '0025', '0619', '0247', '0010', '1107', '1046', '0162', '1305', '1116', '0604', '0827', '0665', '0430', '0116', '0771', '0208', '0086', '0375', '0186', '0248', '0299', '1381', '0284', '0269', '1086', '1290', '0223', '1284', '0177', '0101', '1077', '0156', '0278', '1269', '0238', '0055'])\n",
      "['04', '02', '03', '01']\n"
     ]
    }
   ],
   "source": [
    "rgts = {}\n",
    "for filepath in ATL06_files:\n",
    "    filename = filepath.split('/')[-1]\n",
    "    rgt = filename.split('_')[3][0:4]\n",
    "    track = filename.split('_')[3][4:6]\n",
    "#     print(rgt,track)\n",
    "    if not rgt in rgts.keys():\n",
    "        rgts[rgt] = []\n",
    "        rgts[rgt].append(track)\n",
    "    else:\n",
    "        rgts[rgt].append(track)\n",
    "\n",
    "\n",
    "# all rgt values in our study are are in rgts.keys()\n",
    "print(rgts.keys())\n",
    "\n",
    "# available tracks for each rgt are in rgts[rgt]; ex.:\n",
    "print(rgts['0848'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Revised version of code from Ben Smith to read in the hdf5 files and extract necessary datasets and information\n",
    "def atl06_to_dict(filename, beam, field_dict=None, index=None, epsg=None):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            beam: a string specifying which beam is to be read (ex: gt1l, gt1r, gt2l, etc)\n",
    "            field_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "            index: which entries in each field to read\n",
    "            epsg: an EPSG code specifying a projection (see www.epsg.org).  Good choices are:\n",
    "                for Greenland, 3413 (polar stereographic projection, with Greenland along the Y axis)\n",
    "                for Antarctica, 3031 (polar stereographic projection, centered on the Pouth Pole)\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a numpy array containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    if field_dict is None:\n",
    "        field_dict={None:['latitude','longitude','h_li', 'atl06_quality_summary'],\\\n",
    "                    'ground_track':['x_atc','y_atc'],\\\n",
    "                    'fit_statistics':['dh_fit_dx', 'dh_fit_dy']}\n",
    "    D={}\n",
    "    # below: file_re = regular expression, it will pull apart the regular expression to get the information from the filename\n",
    "    file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n",
    "    with h5py.File(filename,'r') as h5f:\n",
    "        for key in field_dict:\n",
    "            for ds in field_dict[key]:\n",
    "                if key is not None:\n",
    "                    ds_name=beam+'/land_ice_segments/'+key+'/'+ds\n",
    "                else:\n",
    "                    ds_name=beam+'/land_ice_segments/'+ds\n",
    "                if index is not None:\n",
    "                    D[ds]=np.array(h5f[ds_name][index])\n",
    "                else:\n",
    "                    D[ds]=np.array(h5f[ds_name])\n",
    "                if '_FillValue' in h5f[ds_name].attrs:\n",
    "                    bad_vals=D[ds]==h5f[ds_name].attrs['_FillValue']\n",
    "                    D[ds]=D[ds].astype(float)\n",
    "                    D[ds][bad_vals]=np.NaN\n",
    "        D['data_start_utc'] = h5f['/ancillary_data/data_start_utc'][:]\n",
    "        D['delta_time'] = h5f['/' + beam + '/land_ice_segments/delta_time'][:]\n",
    "        D['segment_id'] = h5f['/' + beam + '/land_ice_segments/segment_id'][:]\n",
    "    if epsg is not None:\n",
    "        xy=np.array(pyproj.proj.Proj(epsg)(D['longitude'], D['latitude']))\n",
    "        D['x']=xy[0,:].reshape(D['latitude'].shape)\n",
    "        D['y']=xy[1,:].reshape(D['latitude'].shape)\n",
    "    temp=file_re.search(filename)\n",
    "    D['rgt']=int(temp['rgt'])\n",
    "    D['cycle']=int(temp['cycle'])\n",
    "    D['beam']=beam\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over available rgts and do the correlation processing:\n",
    "\n",
    "Save all results as hdf5 files that can be reloaded and manipulated laver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some functions\n",
    "# MISSING HERE: mask by data quality?\n",
    "def load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, path_to_data, product):\n",
    "    \"\"\" \n",
    "    rgt: repeat ground track number of desired data\n",
    "    smoothing: if true, a centered running avergae filter of smoothing_window_size will be used\n",
    "    smoothing_window_size: how large a smoothing window to use (in meters)\n",
    "    dx: desired spacing \n",
    "    path_to_data: \n",
    "    product: ex., ATL06\n",
    "    \"\"\" \n",
    "    \n",
    "    # hard code these for now:\n",
    "    cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "    beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r'] \n",
    "\n",
    "    ### extract data from all available cycles\n",
    "    x_atc = {}\n",
    "    lats = {}\n",
    "    lons = {}\n",
    "    h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "    h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "    h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "    h_li_diff = {}\n",
    "    times = {}\n",
    "    min_seg_ids = {}\n",
    "    segment_ids = {}\n",
    "\n",
    "    cycles_this_rgt = []\n",
    "    for cycle in cycles: # loop over all available cycles\n",
    "        Di = {}\n",
    "        x_atc[cycle] = {}\n",
    "        lats[cycle] = {}\n",
    "        lons[cycle] = {}\n",
    "        h_li_raw[cycle] = {}\n",
    "        h_li_raw_NoNans[cycle] = {}\n",
    "        h_li[cycle] = {}\n",
    "        h_li_diff[cycle] = {}\n",
    "        times[cycle] = {}\n",
    "        min_seg_ids[cycle] = {}\n",
    "        segment_ids[cycle] = {}\n",
    "\n",
    "\n",
    "        filenames = glob.glob(os.path.join(path_to_data, f'*{product}_*_{rgt}{cycle}*_003*.h5'))\n",
    "        error_count=0\n",
    "\n",
    "\n",
    "        for filename in filenames: # try and load any available files; hopefully is just one\n",
    "            try:\n",
    "                for beam in beams:\n",
    "                    Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                    times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                    # extract h_li and x_atc, and lat/lons for that section                \n",
    "                    x_atc_tmp = Di[filename]['x_atc']\n",
    "                    h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                    lats_tmp = Di[filename]['latitude']\n",
    "                    lons_tmp = Di[filename]['longitude']\n",
    "\n",
    "\n",
    "                    # segment ids:\n",
    "                    seg_ids = Di[filename]['segment_id']\n",
    "                    min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                    #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                    # make a monotonically increasing x vector\n",
    "                    # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                    ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                    x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                    h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    h_full[ind] = h_li_tmp\n",
    "                    lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lats_full[ind] = lats_tmp\n",
    "                    lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lons_full[ind] = lons_tmp\n",
    "\n",
    "                    ## save the segment id's themselves, with gaps filled in\n",
    "                    segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "\n",
    "                    x_atc[cycle][beam] = x_full\n",
    "                    h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                    lons[cycle][beam] = lons_full\n",
    "                    lats[cycle][beam] = lats_full\n",
    "\n",
    "                    ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                    ### interpolate nans in pandas\n",
    "                    # put in dataframe for just this step; eventually rewrite to use only dataframes?              \n",
    "                    data = {'x_full': x_full, 'h_full': h_full}\n",
    "                    df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                    #df.plot(x='x_full',y='h_full')\n",
    "                    # linear interpolation for now\n",
    "                    df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                    h_full_interp = df['h_full'].values\n",
    "                    h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                    # running average smoother /filter\n",
    "                    if smoothing == True:\n",
    "                        h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                        h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                        # differentiate that section of data\n",
    "                        h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    else: \n",
    "                        h_li[cycle][beam] = h_full_interp\n",
    "                        h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "\n",
    "\n",
    "                    #print(len(x_full), len(h_full), len(lats_full), len(seg_ids), len(h_full_interp), len(h_diff))\n",
    "\n",
    "\n",
    "                cycles_this_rgt+=[cycle]\n",
    "            except KeyError as e:\n",
    "                print(f'file {filename} encountered error {e}')\n",
    "                error_count += 1\n",
    "\n",
    "    print('Cycles available: ' + ','.join(cycles_this_rgt))\n",
    "    return x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, \\\n",
    "            times, min_seg_ids, segment_ids, cycles_this_rgt\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choices about processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which cycles to process\n",
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly (this could be future work)\n",
    "\n",
    "### Which beams to process\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "### Which product\n",
    "product = 'ATL06'\n",
    "dx = 20 # x_atc coordinate distance, will be different for different products or processed data\n",
    "\n",
    "# Filter preprocessing: \n",
    "smoothing = True # Whether or not to apply a running average filter\n",
    "smoothing_window_size = int(np.round(100 / dx)) # meters / dx [meters]; this is the number of datapoints to smooth over\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset if dx = 20 for ATL06\n",
    "filt = np.ones(smoothing_window_size) # create the filter to convolve with the data\n",
    "\n",
    "### Control the correlation step:\n",
    "segment_length = 2500 # meters, how wide is the window we are correlating in each step\n",
    "search_width = 800 # meters, how far in front of and behind the window to check for correlation\n",
    "along_track_step = 100 # meters; how much to jump between each consecutivevelocity determination\n",
    "max_percent_nans = 10 # Maximum % of segment length that can be nans and still do the correlation step\n",
    "\n",
    "sub_sample = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the correlation processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interp_xvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3bfd1cf8064d22885024a3ba4e6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(corr_normed)\n",
    "ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "plt.plot(ix_peak, corr_normed[ix_peak], 'r*')\n",
    "plt.plot([ix_peak, ix_peak], [-1, 1], '-')\n",
    "\n",
    "# find trough before and after peak\n",
    "plt.plot([0, len(corr_normed)], [0,0], '-')\n",
    "# plt.plot(corr_normed[1:] - corr_normed[:-1])\n",
    "corr_diff = corr_normed[1:] - corr_normed[:-1]\n",
    "\n",
    "# find next zero crossing: next trough\n",
    "ix_next_trough = np.where(corr_diff[ix_peak:] > 0)[0][0] + ix_peak\n",
    "plt.plot(ix_next_trough, corr_normed[ix_next_trough], 'b*')\n",
    "\n",
    "# find previous zero crossing: previous trough\n",
    "ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak]) < 0)[0][0]\n",
    "plt.plot(ix_prev_trough, corr_normed[ix_prev_trough], 'b*')\n",
    "\n",
    "# select same # points on either side of the peak (min of trough widths)\n",
    "width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])-4\n",
    "\n",
    "dcut = corr_normed[ix_peak - width: ix_peak + width +1]\n",
    "xvec_cut = np.arange(ix_peak - width,  ix_peak + width + 1)\n",
    "plt.plot(xvec_cut, dcut, 'k-')\n",
    "\n",
    "# fit a parabola to the peak\n",
    "fit_coeffs = np.polyfit(xvec_cut, dcut, deg=2, full = True)\n",
    "\n",
    "interp_xvec = np.arange(ix_peak - width,  ix_peak + width + 1, 0.01)\n",
    "fit = np.polyval(p = fit_coeffs[0], x = interp_xvec)\n",
    "plt.plot(interp_xvec, fit, 'r--')\n",
    "\n",
    "# find peak of parabolic fit\n",
    "ix_peak_interp = np.where(fit == np.max(fit))[0][0]\n",
    "# plt.plot(interp_xvec[ix_peak_interp], fit[ix_peak_interp], 'k*')\n",
    "\n",
    "best_lag = interp_xvec[ix_peak_interp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0848, #96 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "tmp\n",
      "Total number of repeat tracks successfully processed = 1\n"
     ]
    }
   ],
   "source": [
    "### Create dictionaries to put info in\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "\n",
    "\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "\n",
    "### Loop over each rgt in the data directory\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir == 96: # this is here in case you want to look at specific rgts; 00848 is 96\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### Determine how many files there are for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### Only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "\n",
    "\n",
    "                ### Load necessary data from all available cycles\n",
    "                x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, times, min_seg_ids, segment_ids, cycles_this_rgt = \\\n",
    "                    load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, datapath, product)\n",
    "                \n",
    "                ### Determine # of possible velocities, given how many cycles are available:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later; We could, for example, do non-consecutive cycles, like 03 and 05\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    \n",
    "                    ### Where to save the results:\n",
    "                    h5_file_out = f'{out_path}rgt{rgt}_veloc{veloc_number}.hdf5'\n",
    "                    \n",
    "                    ### Save some metadata\n",
    "                    with h5py.File(h5_file_out,'w') as f:\n",
    "                        f['dx'] = dx \n",
    "                        f['product'] = product \n",
    "                        f['segment_length'] = segment_length \n",
    "                        f['search_width'] = search_width \n",
    "                        f['along_track_step'] = along_track_step \n",
    "                        f['max_percent_nans'] = max_percent_nans \n",
    "                        f['smoothing'] = smoothing \n",
    "                        f['smoothing_window_size'] = smoothing_window_size \n",
    "                        f['process_date'] = str(Time.now().value) \n",
    "\n",
    "\n",
    "                    ### Which cycles are being processed in the current velocity determination\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    \n",
    "                    ### Timing of each cycle in the current velocity determination\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    ### Elapsed time between cycles\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "                    ### Create dictionaries\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    ### Loop over each beam\n",
    "                    for beam in beams:\n",
    "\n",
    "                        ### Determine x1s, which are the x_atc coordinates at which each cut out window begins\n",
    "                        # To be common between both repeats, the first point x1 needs to be the larger first value between repeats\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            n_segments_this_track = (len(x_atc[cycletmp][beam]) - search_width/dx) / (along_track_step/dx)\n",
    "                            \n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks don't start in the same place\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks do start in the same place\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "                        ### Determine xend, where the x1s vector ends: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= smallest_xatc - search_width/dx)\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### Create vectors to store results in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                                                                                \n",
    "                        ### Entire x_atc vectors for both cycles    \n",
    "                        x_full_t1 = x_atc[cycle1][beam]\n",
    "                        x_full_t2 = x_atc[cycle2][beam]\n",
    "\n",
    "                        ### Loop over x1s, positions along track that each window starts at\n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            \n",
    "                            ### Cut out data: small chunk of data at time t1 (first cycle)\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0] # Index of first point that is greater than x1\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx)) # ix_x1 + number of datapoints within the desired segment length\n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2] # cut out x_atc values, first cycle\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2] # cut out latitude values, first cycle\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2] # cut out longitude values, first cycle\n",
    "                            seg_ids_t1 = segment_ids[cycle1][beam][ix_x1:ix_x2] # cut out segment_ids, first cycle\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # cut out land ice height values, first cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # Find segment midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            ### Cut out data: wider chunk of data at time t2 (second cycle)\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # extend on earlier end by number of indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # extend on later end by number of indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4] # cut out x_atc values, second cycle\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# cut out land ice height values, second cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            ### Determine number of nans in each data chunk\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "                            \n",
    "                            ### Only process if there are fewer than 10% nans in either data chunk:\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # Detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # Normalize both chunks of data, if desired\n",
    "                                # h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "                                # h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                ### Correlate the old and new data\n",
    "                                # We made the second data vector longer than the first, so the valid method returns values\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                ### Normalize correlation function by autocorrelations\n",
    "                                # Normalizing coefficient changes with each step along track; this section determines a changing along track normalizing coefficiant\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but otherwise it yields correlation values above 1...\n",
    "\n",
    "                                ### Create a vector of lags for the correlation function\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                ### Convert lag to distance\n",
    "                                shift_vec = lagvec * dx\n",
    "                                \n",
    "                                if xi == 0:\n",
    "                                    break #tmporary break so I can have a corr_normed vec to play with\n",
    "                                \n",
    "                            \n",
    "                                \n",
    "                                ### ID peak correlation coefficient\n",
    "                                if sub_sample == False:\n",
    "                                    ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                elif sub_sample == True:\n",
    "                                    print('tmp')\n",
    "                                    ix_peak_tmp = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                    ### Figure out peak half width\n",
    "                                    ### Fit a polynomial to the peak\n",
    "                                    \n",
    "                                    \n",
    "                                ### Save correlation coefficient, best lag, velocity, etc at the location of peak correlation coefficient\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                ### If there are too many nans, just save a nan\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                                \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        with h5py.File(h5_file_out, 'a') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            f[beam +'/first_cycle_time'] = str(Time(times[cycle1][beam][0]))\n",
    "                            f[beam +'/second_cycle_time'] = str(Time(times[cycle2][beam][0]))\n",
    "\n",
    "                    ### Record which cycles contributed to these results\n",
    "                    with h5py.File(h5_file_out, 'a') as f:\n",
    "                        f['contributing_cycles'] = ','.join([cycle1,cycle2])\n",
    "\n",
    "            \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            print(e)\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load data, make a map of correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\n",
      "/along_track_step        Dataset {SCALAR}\n",
      "/contributing_cycles     Dataset {SCALAR}\n",
      "/dx                      Dataset {SCALAR}\n",
      "/gt1l                    Group\n",
      "/gt1l/best_lags          Dataset {397}\n",
      "/gt1l/correlation_coefficients Dataset {397}\n",
      "/gt1l/first_cycle_time   Dataset {SCALAR}\n",
      "/gt1l/latitudes          Dataset {397}\n",
      "/gt1l/longitudes         Dataset {397}\n",
      "/gt1l/second_cycle_time  Dataset {SCALAR}\n",
      "/gt1l/segment_ids        Dataset {397}\n",
      "/gt1l/velocities         Dataset {397}\n",
      "/gt1l/x_atc              Dataset {397}\n",
      "/gt1r                    Group\n",
      "/gt1r/best_lags          Dataset {397}\n",
      "/gt1r/correlation_coefficients Dataset {397}\n",
      "/gt1r/first_cycle_time   Dataset {SCALAR}\n",
      "/gt1r/latitudes          Dataset {397}\n",
      "/gt1r/longitudes         Dataset {397}\n",
      "/gt1r/second_cycle_time  Dataset {SCALAR}\n",
      "/gt1r/segment_ids        Dataset {397}\n",
      "/gt1r/velocities         Dataset {397}\n",
      "/gt1r/x_atc              Dataset {397}\n",
      "/gt2l                    Group\n",
      "/gt2l/best_lags          Dataset {304}\n",
      "/gt2l/correlation_coefficients Dataset {304}\n",
      "/gt2l/first_cycle_time   Dataset {SCALAR}\n",
      "/gt2l/latitudes          Dataset {304}\n",
      "/gt2l/longitudes         Dataset {304}\n",
      "/gt2l/second_cycle_time  Dataset {SCALAR}\n",
      "/gt2l/segment_ids        Dataset {304}\n",
      "/gt2l/velocities         Dataset {304}\n",
      "/gt2l/x_atc              Dataset {304}\n",
      "/gt2r                    Group\n",
      "/gt2r/best_lags          Dataset {304}\n",
      "/gt2r/correlation_coefficients Dataset {304}\n",
      "/gt2r/first_cycle_time   Dataset {SCALAR}\n",
      "/gt2r/latitudes          Dataset {304}\n",
      "/gt2r/longitudes         Dataset {304}\n",
      "/gt2r/second_cycle_time  Dataset {SCALAR}\n",
      "/gt2r/segment_ids        Dataset {304}\n",
      "/gt2r/velocities         Dataset {304}\n",
      "/gt2r/x_atc              Dataset {304}\n",
      "/gt3l                    Group\n",
      "/gt3l/best_lags          Dataset {387}\n",
      "/gt3l/correlation_coefficients Dataset {387}\n",
      "/gt3l/first_cycle_time   Dataset {SCALAR}\n",
      "/gt3l/latitudes          Dataset {387}\n",
      "/gt3l/longitudes         Dataset {387}\n",
      "/gt3l/second_cycle_time  Dataset {SCALAR}\n",
      "/gt3l/segment_ids        Dataset {387}\n",
      "/gt3l/velocities         Dataset {387}\n",
      "/gt3l/x_atc              Dataset {387}\n",
      "/gt3r                    Group\n",
      "/gt3r/best_lags          Dataset {386}\n",
      "/gt3r/correlation_coefficients Dataset {386}\n",
      "/gt3r/first_cycle_time   Dataset {SCALAR}\n",
      "/gt3r/latitudes          Dataset {386}\n",
      "/gt3r/longitudes         Dataset {386}\n",
      "/gt3r/second_cycle_time  Dataset {SCALAR}\n",
      "/gt3r/segment_ids        Dataset {386}\n",
      "/gt3r/velocities         Dataset {386}\n",
      "/gt3r/x_atc              Dataset {386}\n",
      "/max_percent_nans        Dataset {SCALAR}\n",
      "/process_date            Dataset {SCALAR}\n",
      "/product                 Dataset {SCALAR}\n",
      "/search_width            Dataset {SCALAR}\n",
      "/segment_length          Dataset {SCALAR}\n",
      "/smoothing               Dataset {SCALAR}\n",
      "/smoothing_window_size   Dataset {SCALAR}\n"
     ]
    }
   ],
   "source": [
    "### What's in each results file\n",
    "\n",
    "!h5ls -r /home/jovyan/shared/surface_velocity/ATL06_out/rgt0589_veloc0.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c6994cd8d4d9cbf8dcfc51d98c49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71f140270414ac398bca0fef367a3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    }
   ],
   "source": [
    "### MOA parameters\n",
    "moa_datapath = '/srv/tutorial-data/land_ice_applications/'\n",
    "spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "# project the coordinates to Antarctic polar stereographic\n",
    "xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "# get the bounds of the projected coordinates \n",
    "XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "epsg=3031 #PS?\n",
    "\n",
    "# Plot MOA with correlation coefficient on top\n",
    "plt.close('all')\n",
    "plt.figure(figsize=[8,8])\n",
    "hax0=plt.gcf().add_subplot(111, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "plt.title('Correlation Coefficient')\n",
    "\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "\n",
    "                h = hax0.scatter(xy[0], xy[1], 0.25, coeffs, vmin = 0, vmax = 1)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "c = plt.colorbar(h)\n",
    "c.set_label('Correlation coefficient (0->1)')\n",
    "outfile = out_path + 'correlation_coefficient.png'\n",
    "plt.savefig(outfile)\n",
    "\n",
    "\n",
    "# Plot MOA with best velocity on top\n",
    "plt.figure(figsize=[8,8])\n",
    "hax2=plt.gcf().add_subplot(111, aspect='equal')\n",
    "MOA.show(ax=hax2,cmap='gray', clim=[14000, 17000])\n",
    "plt.title('Best lag')\n",
    "\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "\n",
    "                h = hax2.scatter(xy[0], xy[1], 0.25, lags, vmin = -10, vmax = 10,cmap='RdBu')\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "c = plt.colorbar(h)\n",
    "c.set_label(f'Best lag (dx) ={dx}')\n",
    "\n",
    "\n",
    "outfile = out_path + 'best_lag.png'\n",
    "plt.savefig(outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Plot results, masked by correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7659cc0971154694bcadfcb7ba4c5fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n",
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    }
   ],
   "source": [
    "### Select a correlation threshold\n",
    "correlation_threshold = 0.65\n",
    "\n",
    "### MOA parameters\n",
    "moa_datapath = '/srv/tutorial-data/land_ice_applications/'\n",
    "spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "\n",
    "lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "# project the coordinates to Antarctic polar stereographic\n",
    "xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "# get the bounds of the projected coordinates \n",
    "XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "epsg=3031\n",
    "\n",
    "# Plot MOA with correlation coefficient and velocity on top\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=[8,8])\n",
    "hax0=fig.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "hax1=fig.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax1,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax0.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax1.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "### Loop over results, load data, plot\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                velocs = f[f'/{beam}/velocities'][()]\n",
    "\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "                ixs0 = coeffs <= correlation_threshold\n",
    "                ixs = coeffs > correlation_threshold\n",
    "\n",
    "                ha = hax0.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'w')#coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                hb = hax1.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'k')#np.zeros(len(coeffs[ixs])))#,cmap='RdBu')\n",
    "                \n",
    "                h0 = hax0.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                h1 = hax1.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -700, vmax = 700,cmap='RdBu')#,cmap='RdBu')\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "c = plt.colorbar(h0, ax = hax0)\n",
    "c.set_label('Correlation coefficient (0 -> 1)')\n",
    "\n",
    "c = plt.colorbar(h1, ax = hax1)\n",
    "c.set_label('Along-track velocity (m/yr)')\n",
    "\n",
    "outfile = out_path + 'results_masked.png'\n",
    "plt.savefig(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate by ascending and descending tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca4fa813b234a74ba56086949ca7952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n",
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f3bba0b22c4b1395103545c6e6520a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n",
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Plot MOA with correlation coefficient and velocity on top, separated by ascending and descending tracks\n",
    "plt.close('all')\n",
    "fig0 = plt.figure(figsize=[8,8])\n",
    "hax0=fig0.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "hax1=fig0.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax1,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax0.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax1.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "fig1 = plt.figure(figsize=[8,8])\n",
    "hax2=fig1.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax2,cmap='gray', clim=[14000, 17000])\n",
    "hax3=fig1.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax3,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax3.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax3.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "### Loop over results, load data, plot\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                velocs = f[f'/{beam}/velocities'][()]\n",
    "\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "                ixs0 = coeffs <= correlation_threshold\n",
    "                ixs = coeffs > correlation_threshold\n",
    "\n",
    "                if np.median(lats[10:20] - lats[9:19]) >=0: # if latitude is increasing\n",
    "                    ha = hax0.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'w')#coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    hb = hax1.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'k')#np.zeros(len(coeffs[ixs])))#,cmap='RdBu')\n",
    "                                \n",
    "                    h0 = hax0.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    h1 = hax1.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -700, vmax = 700,cmap='RdBu')#,cmap='RdBu')\n",
    "                elif np.median(lats[10:20] - lats[9:19]) <0: # if latitude is decreasing\n",
    "                    ha = hax2.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'w')#coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    hb = hax3.scatter(xy[0][ixs0], xy[1][ixs0], 0.05, 'k')#np.zeros(len(coeffs[ixs])))#,cmap='RdBu')\n",
    "                \n",
    "                    h2 = hax2.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    h3 = hax3.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -700, vmax = 700,cmap='RdBu')#,cmap='RdBu')\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "fig0.colorbar(h0, ax = hax0)\n",
    "fig0.colorbar(h1, ax = hax1)\n",
    "fig1.colorbar(h2, ax = hax2)\n",
    "fig1.colorbar(h3, ax = hax3)\n",
    "\n",
    "fig0.suptitle('Descending tracks')\n",
    "fig1.suptitle('Ascending tracks')\n",
    "\n",
    "\n",
    "outfile = out_path + 'results_masked_descending.png'\n",
    "fig0.savefig(outfile)\n",
    "outfile = out_path + 'results_masked_ascending.png'\n",
    "fig1.savefig(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
